{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import os\n",
    "import openai\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import  VectorStoreIndex\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv() # Loading Enviroment Variables\n",
    "embed_model = OpenAIEmbedding(api_key=os.environ.get('OPENAI_API_KEY')) # Setting OpenAI API Key for Embed Model\n",
    "\n",
    "openai.api_key = os.environ.get('OPENAI_API_KEY') # Setting OpenAI API Key \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(input_files=[r\"D:\\Work\\Projects\\Chat-with-Quran\\quran_short.txt\"]).load_data()\n",
    "node_parser = TokenTextSplitter(chunk_size=512, chunk_overlap=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromadb.Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.llamafile import LlamafileEmbedding\n",
    "\n",
    "embedding = LlamafileEmbedding(\n",
    "    base_url=\"http://localhost:8080\",\n",
    ")\n",
    "\n",
    "# pass_embedding = embedding.get_text_embedding_batch(\n",
    "#     [\"This is a passage!\", \"This is another passage\"], show_progress=True\n",
    "# )\n",
    "# print(len(pass_embedding), len(pass_embedding[0]))\n",
    "\n",
    "# query_embedding = embedding.get_query_embedding(nodes[0].text)\n",
    "# print(len(query_embedding))\n",
    "# print(query_embedding[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in nodes:\n",
    "    # print(node.text)\n",
    "    print(embedding.get_text_embedding(node.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding.get_text_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=x, storage_context=storage_context, embed_model=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# Load documents and build index\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[r\"D:\\Work\\Projects\\Chat-with-Quran\\test.txt\"],\n",
    ").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Embeddings Using My Own GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Llama in module llama_cpp.llama:\n",
      "\n",
      "class Llama(builtins.object)\n",
      " |  Llama(model_path: 'str', *, n_gpu_layers: 'int' = 0, split_mode: 'int' = 1, main_gpu: 'int' = 0, tensor_split: 'Optional[List[float]]' = None, vocab_only: 'bool' = False, use_mmap: 'bool' = True, use_mlock: 'bool' = False, kv_overrides: 'Optional[Dict[str, Union[bool, int, float, str]]]' = None, seed: 'int' = 4294967295, n_ctx: 'int' = 512, n_batch: 'int' = 512, n_threads: 'Optional[int]' = None, n_threads_batch: 'Optional[int]' = None, rope_scaling_type: 'Optional[int]' = -1, pooling_type: 'int' = -1, rope_freq_base: 'float' = 0.0, rope_freq_scale: 'float' = 0.0, yarn_ext_factor: 'float' = -1.0, yarn_attn_factor: 'float' = 1.0, yarn_beta_fast: 'float' = 32.0, yarn_beta_slow: 'float' = 1.0, yarn_orig_ctx: 'int' = 0, logits_all: 'bool' = False, embedding: 'bool' = False, offload_kqv: 'bool' = True, flash_attn: 'bool' = False, last_n_tokens_size: 'int' = 64, lora_base: 'Optional[str]' = None, lora_scale: 'float' = 1.0, lora_path: 'Optional[str]' = None, numa: 'Union[bool, int]' = False, chat_format: 'Optional[str]' = None, chat_handler: 'Optional[llama_chat_format.LlamaChatCompletionHandler]' = None, draft_model: 'Optional[LlamaDraftModel]' = None, tokenizer: 'Optional[BaseLlamaTokenizer]' = None, type_k: 'Optional[int]' = None, type_v: 'Optional[int]' = None, verbose: 'bool' = True, **kwargs)\n",
      " |  \n",
      " |  High-level Python wrapper for a llama.cpp model.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, prompt: 'str', suffix: 'Optional[str]' = None, max_tokens: 'Optional[int]' = 16, temperature: 'float' = 0.8, top_p: 'float' = 0.95, min_p: 'float' = 0.05, typical_p: 'float' = 1.0, logprobs: 'Optional[int]' = None, echo: 'bool' = False, stop: 'Optional[Union[str, List[str]]]' = [], frequency_penalty: 'float' = 0.0, presence_penalty: 'float' = 0.0, repeat_penalty: 'float' = 1.1, top_k: 'int' = 40, stream: 'bool' = False, seed: 'Optional[int]' = None, tfs_z: 'float' = 1.0, mirostat_mode: 'int' = 0, mirostat_tau: 'float' = 5.0, mirostat_eta: 'float' = 0.1, model: 'Optional[str]' = None, stopping_criteria: 'Optional[StoppingCriteriaList]' = None, logits_processor: 'Optional[LogitsProcessorList]' = None, grammar: 'Optional[LlamaGrammar]' = None, logit_bias: 'Optional[Dict[str, float]]' = None) -> 'Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]'\n",
      " |      Generate text from a prompt.\n",
      " |      \n",
      " |      Args:\n",
      " |          prompt: The prompt to generate text from.\n",
      " |          suffix: A suffix to append to the generated text. If None, no suffix is appended.\n",
      " |          max_tokens: The maximum number of tokens to generate. If max_tokens <= 0 or None, the maximum number of tokens to generate is unlimited and depends on n_ctx.\n",
      " |          temperature: The temperature to use for sampling.\n",
      " |          top_p: The top-p value to use for nucleus sampling. Nucleus sampling described in academic paper \"The Curious Case of Neural Text Degeneration\" https://arxiv.org/abs/1904.09751\n",
      " |          min_p: The min-p value to use for minimum p sampling. Minimum P sampling as described in https://github.com/ggerganov/llama.cpp/pull/3841\n",
      " |          typical_p: The typical-p value to use for sampling. Locally Typical Sampling implementation described in the paper https://arxiv.org/abs/2202.00666.\n",
      " |          logprobs: The number of logprobs to return. If None, no logprobs are returned.\n",
      " |          echo: Whether to echo the prompt.\n",
      " |          stop: A list of strings to stop generation when encountered.\n",
      " |          frequency_penalty: The penalty to apply to tokens based on their frequency in the prompt.\n",
      " |          presence_penalty: The penalty to apply to tokens based on their presence in the prompt.\n",
      " |          repeat_penalty: The penalty to apply to repeated tokens.\n",
      " |          top_k: The top-k value to use for sampling. Top-K sampling described in academic paper \"The Curious Case of Neural Text Degeneration\" https://arxiv.org/abs/1904.09751\n",
      " |          stream: Whether to stream the results.\n",
      " |          seed: The seed to use for sampling.\n",
      " |          tfs_z: The tail-free sampling parameter. Tail Free Sampling described in https://www.trentonbricken.com/Tail-Free-Sampling/.\n",
      " |          mirostat_mode: The mirostat sampling mode.\n",
      " |          mirostat_tau: The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.\n",
      " |          mirostat_eta: The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.\n",
      " |          model: The name to use for the model in the completion object.\n",
      " |          stopping_criteria: A list of stopping criteria to use.\n",
      " |          logits_processor: A list of logits processors to use.\n",
      " |          grammar: A grammar to use for constrained sampling.\n",
      " |          logit_bias: A logit bias to use.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the requested tokens exceed the context window.\n",
      " |          RuntimeError: If the prompt fails to tokenize or the model fails to evaluate the prompt.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Response object containing the generated text.\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __init__(self, model_path: 'str', *, n_gpu_layers: 'int' = 0, split_mode: 'int' = 1, main_gpu: 'int' = 0, tensor_split: 'Optional[List[float]]' = None, vocab_only: 'bool' = False, use_mmap: 'bool' = True, use_mlock: 'bool' = False, kv_overrides: 'Optional[Dict[str, Union[bool, int, float, str]]]' = None, seed: 'int' = 4294967295, n_ctx: 'int' = 512, n_batch: 'int' = 512, n_threads: 'Optional[int]' = None, n_threads_batch: 'Optional[int]' = None, rope_scaling_type: 'Optional[int]' = -1, pooling_type: 'int' = -1, rope_freq_base: 'float' = 0.0, rope_freq_scale: 'float' = 0.0, yarn_ext_factor: 'float' = -1.0, yarn_attn_factor: 'float' = 1.0, yarn_beta_fast: 'float' = 32.0, yarn_beta_slow: 'float' = 1.0, yarn_orig_ctx: 'int' = 0, logits_all: 'bool' = False, embedding: 'bool' = False, offload_kqv: 'bool' = True, flash_attn: 'bool' = False, last_n_tokens_size: 'int' = 64, lora_base: 'Optional[str]' = None, lora_scale: 'float' = 1.0, lora_path: 'Optional[str]' = None, numa: 'Union[bool, int]' = False, chat_format: 'Optional[str]' = None, chat_handler: 'Optional[llama_chat_format.LlamaChatCompletionHandler]' = None, draft_model: 'Optional[LlamaDraftModel]' = None, tokenizer: 'Optional[BaseLlamaTokenizer]' = None, type_k: 'Optional[int]' = None, type_v: 'Optional[int]' = None, verbose: 'bool' = True, **kwargs)\n",
      " |      Load a llama.cpp model from `model_path`.\n",
      " |      \n",
      " |      Examples:\n",
      " |          Basic usage\n",
      " |      \n",
      " |          >>> import llama_cpp\n",
      " |          >>> model = llama_cpp.Llama(\n",
      " |          ...     model_path=\"path/to/model\",\n",
      " |          ... )\n",
      " |          >>> print(model(\"The quick brown fox jumps \", stop=[\".\"])[\"choices\"][0][\"text\"])\n",
      " |          the lazy dog\n",
      " |      \n",
      " |          Loading a chat model\n",
      " |      \n",
      " |          >>> import llama_cpp\n",
      " |          >>> model = llama_cpp.Llama(\n",
      " |          ...     model_path=\"path/to/model\",\n",
      " |          ...     chat_format=\"llama-2\",\n",
      " |          ... )\n",
      " |          >>> print(model.create_chat_completion(\n",
      " |          ...     messages=[{\n",
      " |          ...         \"role\": \"user\",\n",
      " |          ...         \"content\": \"what is the meaning of life?\"\n",
      " |          ...     }]\n",
      " |          ... ))\n",
      " |      \n",
      " |      Args:\n",
      " |          model_path: Path to the model.\n",
      " |          n_gpu_layers: Number of layers to offload to GPU (-ngl). If -1, all layers are offloaded.\n",
      " |          split_mode: How to split the model across GPUs. See llama_cpp.LLAMA_SPLIT_* for options.\n",
      " |          main_gpu: main_gpu interpretation depends on split_mode: LLAMA_SPLIT_NONE: the GPU that is used for the entire model. LLAMA_SPLIT_ROW: the GPU that is used for small tensors and intermediate results. LLAMA_SPLIT_LAYER: ignored\n",
      " |          tensor_split: How split tensors should be distributed across GPUs. If None, the model is not split.\n",
      " |          vocab_only: Only load the vocabulary no weights.\n",
      " |          use_mmap: Use mmap if possible.\n",
      " |          use_mlock: Force the system to keep the model in RAM.\n",
      " |          kv_overrides: Key-value overrides for the model.\n",
      " |          seed: RNG seed, -1 for random\n",
      " |          n_ctx: Text context, 0 = from model\n",
      " |          n_batch: Prompt processing maximum batch size\n",
      " |          n_threads: Number of threads to use for generation\n",
      " |          n_threads_batch: Number of threads to use for batch processing\n",
      " |          rope_scaling_type: RoPE scaling type, from `enum llama_rope_scaling_type`. ref: https://github.com/ggerganov/llama.cpp/pull/2054\n",
      " |          pooling_type: Pooling type, from `enum llama_pooling_type`.\n",
      " |          rope_freq_base: RoPE base frequency, 0 = from model\n",
      " |          rope_freq_scale: RoPE frequency scaling factor, 0 = from model\n",
      " |          yarn_ext_factor: YaRN extrapolation mix factor, negative = from model\n",
      " |          yarn_attn_factor: YaRN magnitude scaling factor\n",
      " |          yarn_beta_fast: YaRN low correction dim\n",
      " |          yarn_beta_slow: YaRN high correction dim\n",
      " |          yarn_orig_ctx: YaRN original context size\n",
      " |          logits_all: Return logits for all tokens, not just the last token. Must be True for completion to return logprobs.\n",
      " |          embedding: Embedding mode only.\n",
      " |          offload_kqv: Offload K, Q, V to GPU.\n",
      " |          flash_attn: Use flash attention.\n",
      " |          last_n_tokens_size: Maximum number of tokens to keep in the last_n_tokens deque.\n",
      " |          lora_base: Optional path to base model, useful if using a quantized base model and you want to apply LoRA to an f16 model.\n",
      " |          lora_path: Path to a LoRA file to apply to the model.\n",
      " |          numa: numa policy\n",
      " |          chat_format: String specifying the chat format to use when calling create_chat_completion.\n",
      " |          chat_handler: Optional chat handler to use when calling create_chat_completion.\n",
      " |          draft_model: Optional draft model to use for speculative decoding.\n",
      " |          tokenizer: Optional tokenizer to override the default tokenizer from llama.cpp.\n",
      " |          verbose: Print verbose output to stderr.\n",
      " |          type_k: KV cache data type for K (default: f16)\n",
      " |          type_v: KV cache data type for V (default: f16)\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the model path does not exist.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A Llama instance.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  create_chat_completion(self, messages: 'List[ChatCompletionRequestMessage]', functions: 'Optional[List[ChatCompletionFunction]]' = None, function_call: 'Optional[ChatCompletionRequestFunctionCall]' = None, tools: 'Optional[List[ChatCompletionTool]]' = None, tool_choice: 'Optional[ChatCompletionToolChoiceOption]' = None, temperature: 'float' = 0.2, top_p: 'float' = 0.95, top_k: 'int' = 40, min_p: 'float' = 0.05, typical_p: 'float' = 1.0, stream: 'bool' = False, stop: 'Optional[Union[str, List[str]]]' = [], seed: 'Optional[int]' = None, response_format: 'Optional[ChatCompletionRequestResponseFormat]' = None, max_tokens: 'Optional[int]' = None, presence_penalty: 'float' = 0.0, frequency_penalty: 'float' = 0.0, repeat_penalty: 'float' = 1.1, tfs_z: 'float' = 1.0, mirostat_mode: 'int' = 0, mirostat_tau: 'float' = 5.0, mirostat_eta: 'float' = 0.1, model: 'Optional[str]' = None, logits_processor: 'Optional[LogitsProcessorList]' = None, grammar: 'Optional[LlamaGrammar]' = None, logit_bias: 'Optional[Dict[str, float]]' = None, logprobs: 'Optional[bool]' = None, top_logprobs: 'Optional[int]' = None) -> 'Union[CreateChatCompletionResponse, Iterator[CreateChatCompletionStreamResponse]]'\n",
      " |      Generate a chat completion from a list of messages.\n",
      " |      \n",
      " |      Args:\n",
      " |          messages: A list of messages to generate a response for.\n",
      " |          functions: A list of functions to use for the chat completion.\n",
      " |          function_call: A function call to use for the chat completion.\n",
      " |          tools: A list of tools to use for the chat completion.\n",
      " |          tool_choice: A tool choice to use for the chat completion.\n",
      " |          temperature: The temperature to use for sampling.\n",
      " |          top_p: The top-p value to use for nucleus sampling. Nucleus sampling described in academic paper \"The Curious Case of Neural Text Degeneration\" https://arxiv.org/abs/1904.09751\n",
      " |          top_k: The top-k value to use for sampling. Top-K sampling described in academic paper \"The Curious Case of Neural Text Degeneration\" https://arxiv.org/abs/1904.09751\n",
      " |          min_p: The min-p value to use for minimum p sampling. Minimum P sampling as described in https://github.com/ggerganov/llama.cpp/pull/3841\n",
      " |          typical_p: The typical-p value to use for sampling. Locally Typical Sampling implementation described in the paper https://arxiv.org/abs/2202.00666.\n",
      " |          stream: Whether to stream the results.\n",
      " |          stop: A list of strings to stop generation when encountered.\n",
      " |          seed: The seed to use for sampling.\n",
      " |          response_format: The response format to use for the chat completion. Use { \"type\": \"json_object\" } to contstrain output to only valid json.\n",
      " |          max_tokens: The maximum number of tokens to generate. If max_tokens <= 0 or None, the maximum number of tokens to generate is unlimited and depends on n_ctx.\n",
      " |          presence_penalty: The penalty to apply to tokens based on their presence in the prompt.\n",
      " |          frequency_penalty: The penalty to apply to tokens based on their frequency in the prompt.\n",
      " |          repeat_penalty: The penalty to apply to repeated tokens.\n",
      " |          tfs_z: The tail-free sampling parameter.\n",
      " |          mirostat_mode: The mirostat sampling mode.\n",
      " |          mirostat_tau: The mirostat sampling tau parameter.\n",
      " |          mirostat_eta: The mirostat sampling eta parameter.\n",
      " |          model: The name to use for the model in the completion object.\n",
      " |          logits_processor: A list of logits processors to use.\n",
      " |          grammar: A grammar to use.\n",
      " |          logit_bias: A logit bias to use.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Generated chat completion or a stream of chat completion chunks.\n",
      " |  \n",
      " |  create_chat_completion_openai_v1(self, *args: 'Any', **kwargs: 'Any')\n",
      " |      Generate a chat completion with return type based on the the OpenAI v1 API.\n",
      " |      \n",
      " |      OpenAI python package is required to use this method.\n",
      " |      \n",
      " |      You can install it with `pip install openai`.\n",
      " |      \n",
      " |      Args:\n",
      " |          *args: Positional arguments to pass to create_chat_completion.\n",
      " |          **kwargs: Keyword arguments to pass to create_chat_completion.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Generated chat completion or a stream of chat completion chunks.\n",
      " |  \n",
      " |  create_completion(self, prompt: 'Union[str, List[int]]', suffix: 'Optional[str]' = None, max_tokens: 'Optional[int]' = 16, temperature: 'float' = 0.8, top_p: 'float' = 0.95, min_p: 'float' = 0.05, typical_p: 'float' = 1.0, logprobs: 'Optional[int]' = None, echo: 'bool' = False, stop: 'Optional[Union[str, List[str]]]' = [], frequency_penalty: 'float' = 0.0, presence_penalty: 'float' = 0.0, repeat_penalty: 'float' = 1.1, top_k: 'int' = 40, stream: 'bool' = False, seed: 'Optional[int]' = None, tfs_z: 'float' = 1.0, mirostat_mode: 'int' = 0, mirostat_tau: 'float' = 5.0, mirostat_eta: 'float' = 0.1, model: 'Optional[str]' = None, stopping_criteria: 'Optional[StoppingCriteriaList]' = None, logits_processor: 'Optional[LogitsProcessorList]' = None, grammar: 'Optional[LlamaGrammar]' = None, logit_bias: 'Optional[Dict[str, float]]' = None) -> 'Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]'\n",
      " |      Generate text from a prompt.\n",
      " |      \n",
      " |      Args:\n",
      " |          prompt: The prompt to generate text from.\n",
      " |          suffix: A suffix to append to the generated text. If None, no suffix is appended.\n",
      " |          max_tokens: The maximum number of tokens to generate. If max_tokens <= 0 or None, the maximum number of tokens to generate is unlimited and depends on n_ctx.\n",
      " |          temperature: The temperature to use for sampling.\n",
      " |          top_p: The top-p value to use for nucleus sampling. Nucleus sampling described in academic paper \"The Curious Case of Neural Text Degeneration\" https://arxiv.org/abs/1904.09751\n",
      " |          min_p: The min-p value to use for minimum p sampling. Minimum P sampling as described in https://github.com/ggerganov/llama.cpp/pull/3841\n",
      " |          typical_p: The typical-p value to use for sampling. Locally Typical Sampling implementation described in the paper https://arxiv.org/abs/2202.00666.\n",
      " |          logprobs: The number of logprobs to return. If None, no logprobs are returned.\n",
      " |          echo: Whether to echo the prompt.\n",
      " |          stop: A list of strings to stop generation when encountered.\n",
      " |          frequency_penalty: The penalty to apply to tokens based on their frequency in the prompt.\n",
      " |          presence_penalty: The penalty to apply to tokens based on their presence in the prompt.\n",
      " |          repeat_penalty: The penalty to apply to repeated tokens.\n",
      " |          top_k: The top-k value to use for sampling. Top-K sampling described in academic paper \"The Curious Case of Neural Text Degeneration\" https://arxiv.org/abs/1904.09751\n",
      " |          stream: Whether to stream the results.\n",
      " |          seed: The seed to use for sampling.\n",
      " |          tfs_z: The tail-free sampling parameter. Tail Free Sampling described in https://www.trentonbricken.com/Tail-Free-Sampling/.\n",
      " |          mirostat_mode: The mirostat sampling mode.\n",
      " |          mirostat_tau: The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.\n",
      " |          mirostat_eta: The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.\n",
      " |          model: The name to use for the model in the completion object.\n",
      " |          stopping_criteria: A list of stopping criteria to use.\n",
      " |          logits_processor: A list of logits processors to use.\n",
      " |          grammar: A grammar to use for constrained sampling.\n",
      " |          logit_bias: A logit bias to use.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the requested tokens exceed the context window.\n",
      " |          RuntimeError: If the prompt fails to tokenize or the model fails to evaluate the prompt.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Response object containing the generated text.\n",
      " |  \n",
      " |  create_embedding(self, input: 'Union[str, List[str]]', model: 'Optional[str]' = None) -> 'CreateEmbeddingResponse'\n",
      " |      Embed a string.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The utf-8 encoded string to embed.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An embedding object.\n",
      " |  \n",
      " |  detokenize(self, tokens: 'List[int]', prev_tokens: 'Optional[List[int]]' = None) -> 'bytes'\n",
      " |      Detokenize a list of tokens.\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens: The list of tokens to detokenize.\n",
      " |          prev_tokens: The list of previous tokens. Offset mapping will be performed if provided\n",
      " |      \n",
      " |      Returns:\n",
      " |          The detokenized string.\n",
      " |  \n",
      " |  embed(self, input: 'Union[str, List[str]]', normalize: 'bool' = False, truncate: 'bool' = True, return_count: 'bool' = False)\n",
      " |      Embed a string.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The utf-8 encoded string to embed.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of embeddings\n",
      " |  \n",
      " |  eval(self, tokens: 'Sequence[int]')\n",
      " |      Evaluate a list of tokens.\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens: The list of tokens to evaluate.\n",
      " |  \n",
      " |  generate(self, tokens: 'Sequence[int]', top_k: 'int' = 40, top_p: 'float' = 0.95, min_p: 'float' = 0.05, typical_p: 'float' = 1.0, temp: 'float' = 0.8, repeat_penalty: 'float' = 1.1, reset: 'bool' = True, frequency_penalty: 'float' = 0.0, presence_penalty: 'float' = 0.0, tfs_z: 'float' = 1.0, mirostat_mode: 'int' = 0, mirostat_tau: 'float' = 5.0, mirostat_eta: 'float' = 0.1, penalize_nl: 'bool' = True, logits_processor: 'Optional[LogitsProcessorList]' = None, stopping_criteria: 'Optional[StoppingCriteriaList]' = None, grammar: 'Optional[LlamaGrammar]' = None) -> 'Generator[int, Optional[Sequence[int]], None]'\n",
      " |      Create a generator of tokens from a prompt.\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> llama = Llama(\"models/ggml-7b.bin\")\n",
      " |          >>> tokens = llama.tokenize(b\"Hello, world!\")\n",
      " |          >>> for token in llama.generate(tokens, top_k=40, top_p=0.95, temp=1.0, repeat_penalty=1.1):\n",
      " |          ...     print(llama.detokenize([token]))\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens: The prompt tokens.\n",
      " |          top_k: The top-k sampling parameter.\n",
      " |          top_p: The top-p sampling parameter.\n",
      " |          temp: The temperature parameter.\n",
      " |          repeat_penalty: The repeat penalty parameter.\n",
      " |          reset: Whether to reset the model state.\n",
      " |      \n",
      " |      Yields:\n",
      " |          The generated tokens.\n",
      " |  \n",
      " |  load_state(self, state: 'LlamaState') -> 'None'\n",
      " |  \n",
      " |  n_ctx(self) -> 'int'\n",
      " |      Return the context window size.\n",
      " |  \n",
      " |  n_embd(self) -> 'int'\n",
      " |      Return the embedding size.\n",
      " |  \n",
      " |  n_vocab(self) -> 'int'\n",
      " |      Return the vocabulary size.\n",
      " |  \n",
      " |  pooling_type(self) -> 'str'\n",
      " |      Return the pooling type.\n",
      " |  \n",
      " |  reset(self)\n",
      " |      Reset the model state.\n",
      " |  \n",
      " |  sample(self, top_k: 'int' = 40, top_p: 'float' = 0.95, min_p: 'float' = 0.05, typical_p: 'float' = 1.0, temp: 'float' = 0.8, repeat_penalty: 'float' = 1.1, frequency_penalty: 'float' = 0.0, presence_penalty: 'float' = 0.0, tfs_z: 'float' = 1.0, mirostat_mode: 'int' = 0, mirostat_eta: 'float' = 0.1, mirostat_tau: 'float' = 5.0, penalize_nl: 'bool' = True, logits_processor: 'Optional[LogitsProcessorList]' = None, grammar: 'Optional[LlamaGrammar]' = None, idx: 'Optional[int]' = None)\n",
      " |      Sample a token from the model.\n",
      " |      \n",
      " |      Args:\n",
      " |          top_k: The top-k sampling parameter.\n",
      " |          top_p: The top-p sampling parameter.\n",
      " |          temp: The temperature parameter.\n",
      " |          repeat_penalty: The repeat penalty parameter.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The sampled token.\n",
      " |  \n",
      " |  save_state(self) -> 'LlamaState'\n",
      " |  \n",
      " |  set_cache(self, cache: 'Optional[BaseLlamaCache]')\n",
      " |      Set the cache.\n",
      " |      \n",
      " |      Args:\n",
      " |          cache: The cache to set.\n",
      " |  \n",
      " |  set_seed(self, seed: 'int')\n",
      " |      Set the random seed.\n",
      " |      \n",
      " |      Args:\n",
      " |          seed: The random seed.\n",
      " |  \n",
      " |  token_bos(self) -> 'int'\n",
      " |      Return the beginning-of-sequence token.\n",
      " |  \n",
      " |  token_eos(self) -> 'int'\n",
      " |      Return the end-of-sequence token.\n",
      " |  \n",
      " |  token_nl(self) -> 'int'\n",
      " |      Return the newline token.\n",
      " |  \n",
      " |  tokenize(self, text: 'bytes', add_bos: 'bool' = True, special: 'bool' = False) -> 'List[int]'\n",
      " |      Tokenize a string.\n",
      " |      \n",
      " |      Args:\n",
      " |          text: The utf-8 encoded string to tokenize.\n",
      " |      \n",
      " |      Raises:\n",
      " |          RuntimeError: If the tokenization failed.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of tokens.\n",
      " |  \n",
      " |  tokenizer(self) -> 'LlamaTokenizer'\n",
      " |      Return the llama tokenizer for this model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_pretrained(repo_id: 'str', filename: 'Optional[str]', local_dir: 'Optional[Union[str, os.PathLike[str]]]' = None, local_dir_use_symlinks: \"Union[bool, Literal['auto']]\" = 'auto', cache_dir: 'Optional[Union[str, os.PathLike[str]]]' = None, **kwargs: 'Any') -> \"'Llama'\" from builtins.type\n",
      " |      Create a Llama model from a pretrained model name or path.\n",
      " |      This method requires the huggingface-hub package.\n",
      " |      You can install it with `pip install huggingface-hub`.\n",
      " |      \n",
      " |      Args:\n",
      " |          repo_id: The model repo id.\n",
      " |          filename: A filename or glob pattern to match the model file in the repo.\n",
      " |          local_dir: The local directory to save the model to.\n",
      " |          local_dir_use_symlinks: Whether to use symlinks when downloading the model.\n",
      " |          **kwargs: Additional keyword arguments to pass to the Llama constructor.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A Llama model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  logits_to_logprobs(logits: 'Union[npt.NDArray[np.single], List]', axis: 'int' = -1) -> 'npt.NDArray[np.single]'\n",
      " |  \n",
      " |  longest_token_prefix(a: 'Sequence[int]', b: 'Sequence[int]')\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  ctx\n",
      " |  \n",
      " |  eval_logits\n",
      " |  \n",
      " |  eval_tokens\n",
      " |  \n",
      " |  model\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import numpy as np\n",
    "\n",
    "def get_text_embedding(llm: Llama, text: str) -> np.array:\n",
    "    embed = np.array(llm.embed(text))\n",
    "    llm.reset()\n",
    "    return embed\n",
    "\n",
    "llm = Llama(model_path=\"./models/llama-2-7b-chat.gguf.q4_0.bin\", n_ctx=2048,main_gpu=1 ,n_batch=1024, logits_all=True, offload_kqv=True, n_gpu_layers=4096, embedding=True, verbose=False)\n",
    "embedding = get_text_embedding(llm, \"what iswhat is going onwhat is going onwhat is going onwhat is going onwhat is going onwhat is going onwhat is going onwhat is going onwhat is going onwhat is going onwhat is going onwhat is going onwhat is going onwhat is going onwhat is going onwhat is going on going on\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = get_text_embedding(llm, \"what iswhat is going onwhat is going onwhat is going onwhat is going onwhat is going onwhat is going onwhat is going onwhat is going onwhat is going onwhat is going onwhat is going onwhat is going onwhat is going onwhat is going onwhat is going onwhat is going on going on\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verbose': False,\n",
       " 'numa': 0,\n",
       " 'model_path': './models/llama-2-7b-chat.gguf.q4_0.bin',\n",
       " 'model_params': <llama_cpp.llama_cpp.llama_model_params at 0x1f92c5faa40>,\n",
       " 'tensor_split': None,\n",
       " '_c_tensor_split': None,\n",
       " 'kv_overrides': None,\n",
       " 'n_batch': 1024,\n",
       " 'n_threads': 6,\n",
       " 'n_threads_batch': 12,\n",
       " 'context_params': <llama_cpp.llama_cpp.llama_context_params at 0x1f92c5fb540>,\n",
       " 'last_n_tokens_size': 64,\n",
       " 'cache': None,\n",
       " 'lora_base': None,\n",
       " 'lora_scale': 1.0,\n",
       " 'lora_path': None,\n",
       " '_model': <llama_cpp._internals._LlamaModel at 0x1f914178460>,\n",
       " 'tokenizer_': <llama_cpp.llama_tokenizer.LlamaTokenizer at 0x1f915b0e260>,\n",
       " '_ctx': <llama_cpp._internals._LlamaContext at 0x1f92c5a0310>,\n",
       " '_batch': <llama_cpp._internals._LlamaBatch at 0x1f92c5a1360>,\n",
       " 'chat_format': 'llama-2',\n",
       " 'chat_handler': None,\n",
       " '_chat_handlers': {},\n",
       " 'draft_model': None,\n",
       " '_n_vocab': 32000,\n",
       " '_n_ctx': 2048,\n",
       " '_token_nl': 13,\n",
       " '_token_eos': 2,\n",
       " '_candidates': <llama_cpp._internals._LlamaTokenDataArray at 0x1f92c5a0760>,\n",
       " 'n_tokens': 0,\n",
       " 'input_ids': array([0, 0, 0, ..., 0, 0, 0], dtype=int32),\n",
       " 'scores': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " '_mirostat_mu': c_float(10.0),\n",
       " 'metadata': {'general.name': 'llama-2-7b-chat.ggmlv3.q8_0.bin',\n",
       "  'general.architecture': 'llama',\n",
       "  'llama.feed_forward_length': '11008',\n",
       "  'general.description': 'converted from legacy GGJTv3 MOSTLY_Q8_0 format',\n",
       "  'llama.embedding_length': '4096',\n",
       "  'tokenizer.ggml.eos_token_id': '2',\n",
       "  'llama.attention.head_count_kv': '32',\n",
       "  'general.file_type': '7',\n",
       "  'llama.context_length': '2048',\n",
       "  'llama.block_count': '32',\n",
       "  'llama.rope.dimension_count': '128',\n",
       "  'llama.attention.head_count': '32',\n",
       "  'llama.attention.layer_norm_rms_epsilon': '0.000010',\n",
       "  'tokenizer.ggml.model': 'llama',\n",
       "  'tokenizer.ggml.unknown_token_id': '0',\n",
       "  'tokenizer.ggml.bos_token_id': '1'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.83235133e-01, -6.35239035e-02,  1.34809501e-02, ...,\n",
       "         9.35018715e-03,  3.25864255e-02, -1.66996662e-03],\n",
       "       [ 1.49950624e+00, -2.68110186e-01,  1.51078308e+00, ...,\n",
       "        -6.48685217e-01, -6.52460158e-01,  2.35607788e-01],\n",
       "       [ 1.23249972e+00, -9.58741009e-01,  2.02021122e+00, ...,\n",
       "        -1.44635916e+00, -9.10994411e-01, -3.15309942e-01],\n",
       "       [ 8.76478434e-01, -2.58417773e+00,  8.67342800e-02, ...,\n",
       "         5.13657331e-01, -1.87663150e+00, -2.28866771e-01],\n",
       "       [ 5.58136129e+00, -3.39582133e+00,  1.99796796e+00, ...,\n",
       "        -1.38415265e+00,  7.11344302e-01, -5.63589096e-01]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'CMAKE_ARGS' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'CMAKE_ARGS' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
